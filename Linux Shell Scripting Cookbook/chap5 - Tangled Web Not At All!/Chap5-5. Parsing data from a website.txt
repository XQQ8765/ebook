Parsing data from a website


It is often useful to parse data from web pages by eliminating unnecessary details. sed and awk are the main tools that we will use for this task. You might have come across a list of access rankings in a grep recipe in the previous chapter Texting and driving; it was generated by parsing the website page http://www.johntorres.net/BoxOfficefemaleList.html. 

Let's see how to parse the same data using text-processing tools.

How to do it...
Let's go through the command sequence used to parse details of actresses from the website:

Code View: Scroll / Show All
$ lynx -dump http://www.johntorres.net/BoxOfficefemaleList.html | \ grep -o "Rank-.*" | \
sed 's/Rank-//; s/\[[0-9]\+\]//' | \
sort -nk 1 |\
awk '
{
for(i=3;i<=NF;i++){ $2=$2" "$i }
printf "%-4s %s\n", $1,$2 ;
}' > actresslist.txt


					  

The output will be as follows:

# Only 3 entries shown. All others omitted due to space limits
1 Keira Knightley
2 Natalie Portman
3 Monica Bellucci


How it works...
Lynx is a command-line web browser; it can dump the text version of the website as we would see in a web browser rather than showing us the raw code. Hence it avoids the job of removing the HTML tags. We parse the lines starting with Rank, using sed as follows: 

sed 's/Rank-//; s/\[[0-9]\+\]//'


These lines could be then sorted according to the ranks. awk is used here to keep the spacing between rank and the name uniform by specifying the width. %-4s specifies a four-character width. All the fields except the first field are concatenated to form a single string as $2.

See also
Basic sed primer of Chapter 4, explains the sed command

Basic awk primer of Chapter 4, explains the awk command

Downloading a web page as formatted plain text, explains the lynx command
